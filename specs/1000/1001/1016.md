---
# ============================================================================
# SPEC METADATA - This entire frontmatter section contains the spec metadata
# ============================================================================

# === IDENTIFICATION ===
id: "1016" # Numeric ID for stable reference
title: "Tiered Storage Management"
type: "task" # prd | epic | feature | task | subtask | bug | spike

# === HIERARCHY ===
parent: "1001" # Parent spec ID
children: [] # Child spec IDs (if any)
epic: "1000" # Root epic ID for this work
domain: "infrastructure" # Business domain

# === WORKFLOW ===
status: "draft" # draft | reviewing | approved | in-progress | testing | done
priority: "medium" # high | medium | low

# === TRACKING ===
created: "2025-08-24" # YYYY-MM-DD
updated: "2025-08-24" # YYYY-MM-DD
due_date: "" # YYYY-MM-DD (optional)
estimated_hours: 4 # Time estimate in hours
actual_hours: 0 # Time spent so far

# === DEPENDENCIES ===
dependencies: ["1013", "1014"] # Must be done before this (spec IDs)
blocks: [] # This blocks these specs (spec IDs)
related: ["1011", "1012", "1015"] # Related but not blocking (spec IDs)

# === IMPLEMENTATION ===
pull_requests: [] # GitHub PR numbers
commits: [] # Key implementation commits
context_file: "" # Implementation journal
files: [
    "scripts/tiered-storage.sh",
    "scripts/nas-archival.sh",
    "scripts/storage-health.sh",
    "scripts/lvm-backup.sh",
    "/etc/systemd/system/tiered-storage.service",
    "/etc/systemd/system/tiered-storage.timer",
    "docs/TIERED_STORAGE_MANAGEMENT.md",
  ] # Key files to modify

# === METADATA ===
tags: [
    "storage-management",
    "automation",
    "tiered-storage",
    "data-lifecycle",
    "archival",
    "backup",
    "monitoring",
    "systemd",
  ] # Searchable tags
effort: "small" # small | medium | large | epic
risk: "medium" # low | medium | high

# ============================================================================
---

# Tiered Storage Management

## Overview

Implement comprehensive automated management for the three-tier storage infrastructure, including data lifecycle management, automated archival processes, cross-tier health monitoring, and intelligent data movement between storage tiers. This feature provides operational excellence for the entire storage ecosystem.

The management layer ensures optimal utilization of each storage tier, automated maintenance operations, and proactive monitoring to prevent capacity and performance issues across the JTS trading system.

## Acceptance Criteria

- [ ] **Cross-Tier Health Monitoring**: Unified health check script monitoring all three storage tiers (Hot/Warm/Cold)
- [ ] **Automated Data Tiering**: Scripts for intelligent data movement between storage tiers based on age and access patterns
- [ ] **LVM Snapshot Management**: Automated LVM snapshot creation and cleanup for hot storage
- [ ] **NAS Archival Automation**: Automated archival of historical data to NAS with proper organization
- [ ] **Log Management**: Automated log migration from hot to warm storage with compression
- [ ] **Backup Lifecycle**: Multi-tier backup strategy with automated progression (LVM â†’ SATA â†’ NAS)
- [ ] **Storage Optimization**: Automated storage optimization across all tiers
- [ ] **Systemd Integration**: Daily automated tiered storage management via systemd timers
- [ ] **Alert System**: Usage alerts and critical threshold notifications across all tiers

## Technical Approach

### Tiered Management Architecture

Design intelligent automation that optimizes data placement and storage utilization:

- **Data Lifecycle Management**: Automated aging and movement based on access patterns
- **Backup Orchestration**: Coordinated backup operations across all storage tiers
- **Health Monitoring**: Unified monitoring and alerting for the entire storage ecosystem
- **Performance Optimization**: Automated maintenance to sustain optimal performance

### Key Components

1. **Multi-Tier Health Monitoring**
   - Unified health checks across Hot (NVMe), Warm (SATA), and Cold (NAS) storage
   - Usage alerts with tier-specific thresholds
   - I/O performance monitoring and trending
   - Network connectivity validation for NAS

2. **Automated Data Lifecycle Management**
   - ClickHouse data aging: Recent (30 days) on NVMe, historical on NAS
   - PostgreSQL archival: Active transactions on NVMe, archived records on NAS
   - Log management: System logs â†’ Warm storage â†’ NAS archival
   - Backup progression: LVM snapshots â†’ SATA dailies â†’ NAS long-term

3. **Intelligent Storage Optimization**
   - Automated storage defragmentation and optimization
   - TRIM operations for SSD longevity
   - Compression optimization for space efficiency
   - Temporary file cleanup across all tiers

### Implementation Steps

1. **Multi-Tier Health Monitoring Script**
   ```bash
   # Create comprehensive health monitoring
   cat > scripts/storage-health.sh << 'EOF'
   #!/bin/bash
   
   echo "ðŸ” JTS Tiered Storage Health Check - $(date)"
   echo "================================================"
   
   # Hot tier (NVMe) status
   echo "ðŸ”¥ Hot Tier (NVMe) Status:"
   if command -v vgdisplay >/dev/null 2>&1; then
       vgdisplay vg_jts | grep -E "(VG Name|VG Status|VG Size|Free)"
       echo -e "\nLogical Volume Usage:"
       df -h | grep -E "(Filesystem|vg_jts)" | column -t
   else
       echo "LVM not available - Hot tier not configured"
   fi
   
   # Warm tier (SATA) status
   echo -e "\nðŸŒ¡ï¸ Warm Tier (SATA) Status:"
   if mountpoint -q "/data/warm-storage"; then
       df -h /data/warm-storage | tail -1
       btrfs filesystem show /data/warm-storage 2>/dev/null || echo "Btrfs info unavailable"
   else
       echo "Warm storage not mounted"
   fi
   
   # Cold tier (NAS) status
   echo -e "\nðŸ§Š Cold Tier (NAS) Status:"
   if mountpoint -q "/mnt/synology"; then
       df -h /mnt/synology | tail -1
       ping -c 2 192.168.1.101 >/dev/null 2>&1 && echo "âœ… NAS reachable" || echo "âŒ NAS unreachable"
   else
       echo "NAS not mounted"
   fi
   
   # Usage alerts across all tiers
   echo -e "\nâš ï¸ Usage Alerts:"
   df -h | awk 'NR>1 && /vg_jts|warm-storage|synology/ {
       gsub(/%/, "", $5);
       if ($5 > 90) print "CRITICAL: " $6 " usage at " $5 "%"
       else if ($5 > 80) print "WARNING: " $6 " usage at " $5 "%"
   }'
   EOF
   
   chmod +x scripts/storage-health.sh
   ```

2. **Automated Data Tiering Script**
   ```bash
   # Create tiered storage management
   cat > scripts/tiered-storage.sh << 'EOF'
   #!/bin/bash
   set -euo pipefail
   
   HOT_TIER="/var/lib"
   WARM_TIER="/data/warm-storage"
   COLD_TIER="/mnt/synology/jts"
   
   migrate_logs_to_warm() {
       echo "ðŸ“Ž Moving old logs to warm storage"
       
       if [[ -d "$WARM_TIER" ]]; then
           # Move logs older than 7 days to warm tier
           find /var/log -name "*.log" -mtime +7 -exec mv {} "$WARM_TIER/logs/" \; 2>/dev/null || true
           
           # Compress moved logs
           find "$WARM_TIER/logs/" -name "*.log" -exec gzip {} \; 2>/dev/null || true
           
           echo "âœ… Log migration to warm tier completed"
       else
           echo "âš ï¸ Warm tier not available - skipping log migration"
       fi
   }
   
   migrate_backups_to_cold() {
       echo "ðŸ“Ž Moving old backups to cold storage"
       
       if [[ -d "$COLD_TIER" && -d "$WARM_TIER" ]]; then
           # Move backups older than 30 days to cold tier
           find "$WARM_TIER/daily-backups/" -mtime +30 -exec mv {} "$COLD_TIER/archives/" \; 2>/dev/null || true
           
           echo "âœ… Backup migration to cold tier completed"
       else
           echo "âš ï¸ Cold or warm tier not available - skipping backup migration"
       fi
   }
   
   cleanup_temp_files() {
       echo "ðŸ§¤ Cleaning up temporary files"
       
       # Clean up temp processing files older than 3 days
       if [[ -d "$WARM_TIER/temp-processing" ]]; then
           find "$WARM_TIER/temp-processing/" -mtime +3 -delete 2>/dev/null || true
       fi
       
       # Clean up Docker temp files if Docker is available
       if command -v docker >/dev/null 2>&1; then
           docker system prune -f --volumes 2>/dev/null || true
       fi
       
       echo "âœ… Temporary file cleanup completed"
   }
   
   case "${1:-check}" in
       migrate)
           migrate_logs_to_warm
           migrate_backups_to_cold
           ;;
       cleanup)
           cleanup_temp_files
           ;;
       all)
           migrate_logs_to_warm
           migrate_backups_to_cold
           cleanup_temp_files
           ;;
       *)
           echo "Usage: $0 {migrate|cleanup|all}"
           exit 1
           ;;
   esac
   EOF
   
   chmod +x scripts/tiered-storage.sh
   ```

3. **NAS Archival Management**
   ```bash
   # Create NAS archival script
   cat > scripts/nas-archival.sh << 'EOF'
   #!/bin/bash
   set -euo pipefail
   
   NAS_BASE="/mnt/synology/jts"
   DATE=$(date +%Y%m%d_%H%M%S)
   
   archive_development_data() {
       echo "ðŸ“‹ Syncing development resources to NAS"
       
       if [[ -d "$NAS_BASE" ]]; then
           # Sync Jupyter notebooks if directory exists
           [[ -d "/home/joohan/notebooks" ]] && rsync -av --delete /home/joohan/notebooks/ "$NAS_BASE/development/notebooks/" || true
           
           # Sync development datasets if directory exists
           [[ -d "/home/joohan/dev/datasets" ]] && rsync -av --delete /home/joohan/dev/datasets/ "$NAS_BASE/development/datasets/" || true
           
           echo "âœ… Development sync completed"
       else
           echo "âš ï¸ NAS not available - skipping development data sync"
       fi
   }
   
   backup_configurations() {
       echo "ðŸ“‹ Backing up system configurations to NAS"
       
       if [[ -d "$NAS_BASE" ]]; then
           mkdir -p "$NAS_BASE/archives/configs/$DATE"
           
           # Backup important configs
           cp -r /etc/fstab "$NAS_BASE/archives/configs/$DATE/" 2>/dev/null || true
           [[ -d "/home/joohan/dev/project-jts/jts/configs" ]] && cp -r /home/joohan/dev/project-jts/jts/configs/ "$NAS_BASE/archives/configs/$DATE/" || true
           
           echo "âœ… Configuration backup completed"
       else
           echo "âš ï¸ NAS not available - skipping configuration backup"
       fi
   }
   
   case "${1:-all}" in
       development)
           archive_development_data
           ;;
       configs)
           backup_configurations
           ;;
       all)
           archive_development_data
           backup_configurations
           ;;
       *)
           echo "Usage: $0 {development|configs|all}"
           exit 1
           ;;
   esac
   EOF
   
   chmod +x scripts/nas-archival.sh
   ```

4. **LVM Backup Management**
   ```bash
   # Create LVM snapshot management
   cat > scripts/lvm-backup.sh << 'EOF'
   #!/bin/bash
   
   DATE=$(date +%Y%m%d_%H%M%S)
   VG_NAME="vg_jts"
   
   create_snapshots() {
       echo "ðŸ“¸ Creating LVM snapshots - $DATE"
       
       if command -v lvcreate >/dev/null 2>&1 && vgs "$VG_NAME" >/dev/null 2>&1; then
           # Create snapshots for critical data
           lvcreate -L5G -s -n snap_postgres_$DATE /dev/$VG_NAME/lv_postgres 2>/dev/null || echo "PostgreSQL snapshot failed"
           lvcreate -L10G -s -n snap_clickhouse_$DATE /dev/$VG_NAME/lv_clickhouse 2>/dev/null || echo "ClickHouse snapshot failed"
           lvcreate -L2G -s -n snap_mongodb_$DATE /dev/$VG_NAME/lv_mongodb 2>/dev/null || echo "MongoDB snapshot failed"
           
           echo "âœ… Snapshots created successfully"
       else
           echo "âš ï¸ LVM not available - skipping snapshot creation"
       fi
   }
   
   cleanup_old_snapshots() {
       echo "ðŸ§¹ Cleaning up snapshots older than 7 days"
       
       if command -v lvs >/dev/null 2>&1 && vgs "$VG_NAME" >/dev/null 2>&1; then
           # Find and remove old snapshots
           lvs --noheadings -o lv_name "$VG_NAME" 2>/dev/null | grep "snap_" | while read snapshot; do
               snapshot=$(echo $snapshot | xargs)
               if [[ -n "$snapshot" ]]; then
                   # Simple age-based cleanup (remove snapshots with old naming pattern)
                   if [[ "$snapshot" =~ snap_.*_[0-9]{8}_[0-9]{6} ]]; then
                       echo "Removing old snapshot: $snapshot"
                       lvremove -f "/dev/$VG_NAME/$snapshot" 2>/dev/null || echo "Failed to remove $snapshot"
                   fi
               fi
           done
           
           echo "âœ… Snapshot cleanup completed"
       else
           echo "âš ï¸ LVM not available - skipping snapshot cleanup"
       fi
   }
   
   case "${1:-create}" in
       create)
           create_snapshots
           ;;
       cleanup)
           cleanup_old_snapshots
           ;;
       *)
           echo "Usage: $0 {create|cleanup}"
           exit 1
           ;;
   esac
   EOF
   
   chmod +x scripts/lvm-backup.sh
   ```

5. **Systemd Automation Setup**
   ```bash
   # Create tiered storage service
   cat > /etc/systemd/system/tiered-storage.service << 'EOF'
   [Unit]
   Description=JTS Tiered Storage Management
   After=network.target
   
   [Service]
   Type=oneshot
   ExecStart=/home/joohan/dev/project-jts/jts/scripts/tiered-storage.sh all
   User=root
   StandardOutput=journal
   StandardError=journal
   EOF
   
   # Create daily timer
   cat > /etc/systemd/system/tiered-storage.timer << 'EOF'
   [Unit]
   Description=Run tiered storage management daily
   
   [Timer]
   OnCalendar=daily
   Persistent=true
   
   [Install]
   WantedBy=timers.target
   EOF
   
   # Enable and start timer
   systemctl daemon-reload
   systemctl enable tiered-storage.timer
   systemctl start tiered-storage.timer
   
   # Verify timer status
   systemctl status tiered-storage.timer --no-pager
   ```

## Dependencies

**Required Before Implementation:**
- **Feature 1013**: Warm Storage (SATA) Setup for backup and log management
- **Feature 1014**: Cold Storage (NAS) Integration for archival operations

**Optional Enhancements:**
- **Feature 1011**: Hot Storage provides LVM snapshot capabilities
- **Feature 1012**: Database Mount Integration enables database-specific backup operations

## Testing Plan

- **Health Monitoring**: Test cross-tier health checks and alert generation
- **Data Migration**: Validate automated movement of logs and backups between tiers
- **LVM Snapshots**: Test snapshot creation and cleanup procedures
- **NAS Archival**: Verify automated archival to NAS with proper organization
- **Systemd Integration**: Test daily automated execution via systemd timers
- **Error Handling**: Validate graceful handling when storage tiers are unavailable
- **Performance Impact**: Ensure management operations don't impact trading system performance
- **Recovery Testing**: Test recovery procedures using tiered backups

## Claude Code Instructions

```
When implementing tiered storage management:

OPERATIONAL EXCELLENCE:
1. All scripts must handle missing storage tiers gracefully
2. Include comprehensive error handling and logging
3. Provide clear status messages for all operations
4. Design for unattended operation via systemd timers

DATA LIFECYCLE MANAGEMENT:
1. Implement intelligent data aging based on access patterns
2. Ensure data consistency during tier migrations
3. Maintain backup integrity across all tiers
4. Provide rollback capabilities for data movement

AUTOMATION DESIGN:
1. Use systemd timers for reliable scheduling
2. Include proper logging for all automated operations
3. Design scripts to be idempotent (safe to run multiple times)
4. Provide manual override capabilities for emergency situations

MONITORING INTEGRATION:
1. Unified health checks across all storage tiers
2. Proactive alerting for capacity and performance issues
3. Integration with existing monitoring infrastructure
4. Clear status reporting for operational visibility

SAFETY CONSIDERATIONS:
1. Never move data without verification of destination availability
2. Include data integrity checks in movement operations
3. Maintain multiple backup copies during tier transitions
4. Provide emergency stop procedures for automated operations
```

## Notes

### Management Benefits
- **Operational Automation**: Reduces manual storage management overhead
- **Proactive Monitoring**: Prevents storage capacity and performance issues
- **Data Lifecycle**: Optimizes storage utilization through intelligent data placement
- **Backup Orchestration**: Ensures comprehensive backup coverage across all tiers

### Integration Considerations
- **Cross-Tier Dependencies**: Requires warm and cold storage tiers to be functional
- **Graceful Degradation**: Continues operation even when some tiers are unavailable
- **Performance Awareness**: Scheduled during low-activity periods to minimize impact
- **Monitoring Integration**: Provides foundation for advanced monitoring systems

## Status Updates

- **2025-08-24**: Feature specification created as management automation component extracted from monolithic storage spec