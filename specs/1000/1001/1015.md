---
# ============================================================================
# SPEC METADATA - This entire frontmatter section contains the spec metadata
# ============================================================================

# === IDENTIFICATION ===
id: "1015" # Numeric ID for stable reference
title: "Storage Performance Optimization"
type: "task" # prd | epic | feature | task | subtask | bug | spike

# === HIERARCHY ===
parent: "1001" # Parent spec ID
children: [] # Child spec IDs (if any)
epic: "1000" # Root epic ID for this work
domain: "infrastructure" # Business domain

# === WORKFLOW ===
status: "draft" # draft | reviewing | approved | in-progress | testing | done
priority: "medium" # high | medium | low

# === TRACKING ===
created: "2025-08-24" # YYYY-MM-DD
updated: "2025-08-24" # YYYY-MM-DD
due_date: "" # YYYY-MM-DD (optional)
estimated_hours: 2 # Time estimate in hours
actual_hours: 0 # Time spent so far

# === DEPENDENCIES ===
dependencies: ["1011"] # Must be done before this (spec IDs)
blocks: [] # This blocks these specs (spec IDs)
related: ["1013", "1014"] # Related but not blocking (spec IDs)

# === IMPLEMENTATION ===
pull_requests: [] # GitHub PR numbers
commits: [] # Key implementation commits
context_file: "" # Implementation journal
files: [
    "/etc/udev/rules.d/60-ssd-scheduler.rules",
    "/etc/systemd/system/fstrim-all.service",
    "/etc/systemd/system/fstrim-all.timer",
    "scripts/performance-benchmark.sh",
    "scripts/ssd-optimization.sh",
    "docs/PERFORMANCE_OPTIMIZATION.md",
  ] # Key files to modify

# === METADATA ===
tags: [
    "performance",
    "optimization",
    "ssd",
    "trim",
    "io-scheduler",
    "benchmarking",
    "tuning",
    "systemd",
  ] # Searchable tags
effort: "small" # small | medium | large | epic
risk: "low" # low | medium | high

# ============================================================================
---

# Storage Performance Optimization

## Overview

Implement comprehensive performance optimizations across all storage tiers to maximize I/O performance for the JTS trading system. This includes SSD-specific optimizations (I/O schedulers, TRIM support), filesystem tuning, and automated performance maintenance to ensure sustained high-performance operation under trading workloads.

These optimizations are critical for maintaining the sub-millisecond response times required for algorithmic trading operations and ensuring long-term storage performance stability.

## Acceptance Criteria

- [ ] **I/O Scheduler Optimization**: NVMe drives configured with 'none' scheduler for minimal CPU overhead
- [ ] **TRIM Support Configuration**: Automated weekly TRIM operations for all NVMe filesystems
- [ ] **SSD Longevity Optimization**: Udev rules for automatic SSD-specific optimizations
- [ ] **Systemd Timer Setup**: Automated maintenance timers for storage optimization
- [ ] **Performance Benchmarking**: Baseline performance tests for sequential and random I/O across all storage tiers
- [ ] **Optimization Scripts**: Automated scripts for ongoing performance tuning and maintenance
- [ ] **Performance Monitoring**: Basic performance monitoring and alerting for storage bottlenecks
- [ ] **Documentation**: Complete performance tuning guide and troubleshooting procedures

## Technical Approach

### Performance Optimization Strategy

Implement tier-specific optimizations to maximize performance across the entire storage infrastructure:

- **Hot Storage (NVMe)**: Maximum IOPS with minimal latency for real-time operations
- **Warm Storage (SATA)**: Optimized sequential I/O for backup and log operations  
- **Cold Storage (NAS)**: Network-optimized bulk transfer performance
- **Cross-Tier**: Coordinated optimizations that don't conflict between storage types

### Key Components

1. **SSD-Specific Optimizations**
   - I/O scheduler configuration for NVMe drives
   - TRIM/discard support for SSD longevity
   - Udev rules for automatic optimization application
   - Performance monitoring and alerting

2. **Filesystem Performance Tuning**
   - Database-specific mount options already configured in Feature 1011
   - Additional runtime tuning for sustained performance
   - Performance monitoring and adjustment procedures

3. **Automated Maintenance**
   - Weekly TRIM operations for all NVMe filesystems
   - Performance benchmark automation
   - Storage health monitoring integration

### Implementation Steps

1. **SSD I/O Scheduler Configuration**
   ```bash
   # Configure NVMe I/O scheduler for optimal performance
   echo 'ACTION=="add|change", KERNEL=="nvme0n1", ATTR{queue/scheduler}="none"' > /etc/udev/rules.d/60-ssd-scheduler.rules
   
   # Apply immediately without reboot
   echo none > /sys/block/nvme0n1/queue/scheduler
   
   # Verify scheduler configuration
   cat /sys/block/nvme0n1/queue/scheduler
   ```

2. **TRIM Support Configuration**
   ```bash
   # Configure automatic TRIM support
   echo 'ACTION=="add|change", KERNEL=="nvme0n1", ATTR{queue/discard_max_bytes}!="0", RUN+="/sbin/fstrim -v /"' >> /etc/udev/rules.d/60-ssd-scheduler.rules
   
   # Create TRIM service for all NVMe mounts
   cat > /etc/systemd/system/fstrim-all.service << 'EOF'
   [Unit]
   Description=Discard unused filesystem blocks on all NVMe filesystems
   
   [Service]
   Type=oneshot
   ExecStart=/bin/bash -c 'for mount in /var/lib/postgresql /var/lib/clickhouse /var/lib/kafka /var/lib/mongodb /var/lib/redis /var/lib/docker-jts /data/local-backup; do /sbin/fstrim -v "$mount"; done'
   EOF
   
   # Create weekly TRIM timer
   cat > /etc/systemd/system/fstrim-all.timer << 'EOF'
   [Unit]
   Description=Run fstrim weekly for SSD longevity
   
   [Timer]
   OnCalendar=weekly
   Persistent=true
   
   [Install]
   WantedBy=timers.target
   EOF
   
   # Enable and start TRIM timer
   systemctl enable fstrim-all.timer
   systemctl start fstrim-all.timer
   ```

3. **Performance Benchmarking Setup**
   ```bash
   # Create comprehensive performance benchmark script
   cat > scripts/performance-benchmark.sh << 'EOF'
   #!/bin/bash
   
   echo "ðŸš€ JTS Storage Performance Benchmarking"
   echo "======================================="
   
   test_sequential_performance() {
       local mount_point=$1
       local tier_name=$2
       
       echo "Testing $tier_name sequential I/O on $mount_point"
       
       # Sequential write test
       echo "  Sequential Write:"
       dd if=/dev/zero of="$mount_point/seq_write_test" bs=1M count=1000 oflag=direct 2>&1 | tail -1
       
       # Sequential read test
       echo "  Sequential Read:"
       dd if="$mount_point/seq_write_test" of=/dev/null bs=1M 2>&1 | tail -1
       
       # Cleanup
       rm -f "$mount_point/seq_write_test"
   }
   
   test_random_iops() {
       local mount_point=$1
       local tier_name=$2
       
       echo "Testing $tier_name random IOPS on $mount_point"
       
       # Random read IOPS test (requires fio)
       if command -v fio >/dev/null 2>&1; then
           fio --name=random_test --ioengine=libaio --iodepth=32 --rw=randread --bs=4k \
               --direct=1 --size=500M --numjobs=1 --runtime=30 --group_reporting \
               --filename="$mount_point/random_test" | grep -E "(read|IOPS)"
           rm -f "$mount_point/random_test"
       else
           echo "  fio not installed - skipping random IOPS test"
       fi
   }
   
   # Test hot storage performance
   echo "ðŸ”¥ Hot Storage (NVMe) Performance Tests:"
   for mount in /var/lib/postgresql /var/lib/clickhouse /var/lib/kafka; do
       if mountpoint -q "$mount"; then
           echo -e "\nðŸ“Š Testing $mount"
           test_sequential_performance "$mount" "Hot"
           test_random_iops "$mount" "Hot"
       fi
   done
   
   # Test warm storage if available
   if mountpoint -q "/data/warm-storage"; then
       echo -e "\nðŸŒ¡ï¸ Warm Storage (SATA) Performance Tests:"
       test_sequential_performance "/data/warm-storage" "Warm"
   fi
   
   # Test cold storage if available
   if mountpoint -q "/mnt/synology"; then
       echo -e "\nðŸ§Š Cold Storage (NAS) Performance Tests:"
       test_sequential_performance "/mnt/synology/jts/development" "Cold"
   fi
   EOF
   
   chmod +x scripts/performance-benchmark.sh
   ```

4. **Optimization Validation and Monitoring**
   ```bash
   # Create optimization monitoring script
   cat > scripts/ssd-optimization.sh << 'EOF'
   #!/bin/bash
   
   echo "âš™ï¸ SSD Optimization Status Check"
   echo "==============================="
   
   # Check I/O scheduler
   echo "ðŸ“Š I/O Scheduler Status:"
   echo "NVMe0n1: $(cat /sys/block/nvme0n1/queue/scheduler 2>/dev/null || echo 'Not found')"
   
   # Check TRIM support
   echo -e "\nâœ‚ï¸ TRIM Support Status:"
   lsblk -D /dev/nvme0n1 2>/dev/null || echo "TRIM status unavailable"
   
   # Check systemd timers
   echo -e "\nâ° Automated Maintenance Status:"
   systemctl status fstrim-all.timer --no-pager -l
   
   # Show next TRIM execution
   echo -e "\nðŸ“… Next TRIM Execution:"
   systemctl list-timers fstrim-all.timer --no-pager
   
   # Performance summary
   echo -e "\nðŸ“ˆ Quick Performance Check:"
   iostat -x 1 1 2>/dev/null | grep nvme0n1 | tail -1 || echo "iostat not available"
   EOF
   
   chmod +x scripts/ssd-optimization.sh
   ```

## Dependencies

**Required Before Implementation:**
- **Feature 1011**: Hot Storage (NVMe) Foundation must be completed with mounted filesystems

**No Blocking Dependencies**: This feature enhances performance but doesn't block other features

## Testing Plan

- **I/O Scheduler Validation**: Verify NVMe scheduler is set to 'none' for optimal performance
- **TRIM Functionality**: Test manual and automated TRIM operations
- **Performance Benchmarking**: Run comprehensive I/O tests on all storage tiers
- **Optimization Validation**: Confirm all optimization settings are active and effective
- **Timer Functionality**: Verify systemd timers are properly scheduled and executing
- **Performance Monitoring**: Test performance monitoring and alerting capabilities
- **Regression Testing**: Ensure optimizations don't negatively impact functionality

## Claude Code Instructions

```
When implementing storage performance optimization:

PERFORMANCE FOCUS:
1. This feature enhances the foundation provided by Feature 1011
2. Focus on measurable performance improvements
3. Benchmark before and after optimization application
4. Document performance gains for validation

SSD OPTIMIZATION PRIORITY:
1. I/O scheduler optimization has immediate impact
2. TRIM support critical for long-term performance
3. Mount options already optimized in Feature 1011
4. Focus on system-level optimizations

AUTOMATION SETUP:
1. Use systemd timers for reliable automated maintenance
2. Configure appropriate scheduling (weekly TRIM)
3. Include comprehensive logging for maintenance operations
4. Set up monitoring for automated task execution

TESTING APPROACH:
1. Establish performance baselines before optimization
2. Measure improvement after each optimization
3. Test under realistic trading system workloads
4. Validate optimization persistence across reboots

LOW RISK IMPLEMENTATION:
1. All optimizations are reversible
2. No data loss risk with performance tuning
3. Can be applied incrementally and tested
4. Safe to implement without extensive backup procedures
```

## Notes

### Performance Benefits
- **Immediate Impact**: I/O scheduler changes provide instant performance improvement
- **Long-term Stability**: TRIM operations maintain SSD performance over time
- **Measurable Results**: Performance benchmarking validates optimization effectiveness
- **Automated Maintenance**: Reduces operational overhead while maintaining performance

### Trading System Impact
- **Latency Reduction**: Optimized I/O paths reduce trading system latency
- **Sustained Performance**: Automated maintenance prevents performance degradation
- **Reliability**: Consistent performance under varying workloads
- **Scalability**: Optimization framework supports future performance enhancements

## Status Updates

- **2025-08-24**: Feature specification created as performance optimization component extracted from monolithic storage spec