---
# ============================================================================
# SPEC METADATA - This entire frontmatter section contains the spec metadata
# ============================================================================

# === IDENTIFICATION ===
id: '1006' # Numeric ID for stable reference
title: 'Message Queue Setup'
type: 'feature' # prd | epic | feature | task | subtask | bug | spike

# === HIERARCHY ===
parent: '1000' # Parent spec ID
children: [] # Child spec IDs (if any)
epic: '1000' # Root epic ID for this work
domain: 'infrastructure' # Business domain

# === WORKFLOW ===
status: 'draft' # draft | reviewing | approved | in-progress | testing | done
priority: 'high' # high | medium | low

# === TRACKING ===
created: '2025-08-24' # YYYY-MM-DD
updated: '2025-08-24' # YYYY-MM-DD
due_date: '' # YYYY-MM-DD (optional)
estimated_hours: 16 # Time estimate in hours
actual_hours: 0 # Time spent so far

# === DEPENDENCIES ===
dependencies: ['1001'] # Must be done before this (spec IDs)
blocks: ['1007', '2000', '3000'] # This blocks these specs (spec IDs)
related: ['1005'] # Related but not blocking (spec IDs)

# === IMPLEMENTATION ===
branch: 'feature/1006-message-queue-setup' # Git branch name
files: ['infrastructure/kafka/', 'docker-compose.kafka.yml', 'libs/shared/messaging/', 'schemas/protobuf/', 'scripts/kafka-setup/'] # Key files to modify

# === METADATA ===
tags: ['kafka', 'messaging', 'event-streaming', 'protobuf', 'topics', 'partitioning', 'producers', 'consumers'] # Searchable tags
effort: 'large' # small | medium | large | epic
risk: 'medium' # low | medium | high

# ============================================================================
---

# Message Queue Setup

## Overview

Implement a comprehensive Apache Kafka cluster configuration for the JTS automated trading system, focusing on high-throughput event streaming architecture. This feature establishes the core messaging backbone with optimized topic design, Protocol Buffers serialization, producer/consumer patterns, and event streaming capabilities specifically designed for real-time trading operations.

## Acceptance Criteria

- [ ] **Kafka Cluster Configuration**: Production-ready 3-broker Kafka cluster with Zookeeper, optimized for trading workloads with proper resource allocation and 4TB SSD storage configuration
- [ ] **Schema Registry Integration**: Confluent Schema Registry deployment with backward compatibility for schema evolution
- [ ] **Topic Design & Partitioning**: Well-architected topic structure with strategic partitioning for Korean (KRX) and cryptocurrency markets, order events, strategy signals, and system notifications
- [ ] **Protocol Buffers Integration**: Complete Protocol Buffers schema definitions with code generation for TypeScript and efficient binary serialization
- [ ] **Producer Patterns**: High-performance producer implementations with batching, compression (LZ4), idempotent writes, and error handling strategies
- [ ] **Consumer Patterns**: Robust consumer groups with offset management, rebalancing strategies, and parallel processing capabilities  
- [ ] **Event Streaming Architecture**: Stream processing topology for real-time data transformation, aggregation, and event correlation
- [ ] **Performance Optimization**: Kafka cluster tuned for sub-millisecond latency with JVM heap optimization (8GB per broker), kernel tuning, and network configuration
- [ ] **Monitoring & Observability**: Kafka UI for visual monitoring, JMX metrics, Prometheus exporter integration, and comprehensive health checks
- [ ] **Storage Migration Strategy**: Clear migration path from local storage to 4TB NVMe SSD with XFS filesystem optimization
- [ ] **Development Tools**: Kafka admin tools, testing utilities, backup scripts, and local development environment setup

## Technical Approach

### Event Streaming Architecture Design

Design a highly scalable event streaming architecture that supports real-time trading operations with guaranteed message delivery, ordered processing, and low-latency distribution. The architecture leverages Kafka's distributed commit log model optimized for financial data processing requirements.

#### System Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Event Streaming Layer                             │
├─────────────────┬─────────────────┬─────────────────┬─────────────────────┤
│  Market Data    │  Order Events   │ Strategy Events │  System Events      │
│     Topics      │     Topics      │     Topics      │     Topics          │
├─────────────────┼─────────────────┼─────────────────┼─────────────────────┤
│ • price-ticks   │ • order-created │ • signals       │ • health-checks     │
│ • orderbook     │ • order-filled  │ • performance   │ • alerts            │
│ • trades        │ • order-cancel  │ • backtest      │ • notifications     │
│ • ohlcv         │ • order-reject  │ • risk-breach   │ • audit-logs        │
└─────────────────┴─────────────────┴─────────────────┴─────────────────────┘
                                      │
              ┌───────────────────────────────────────────┐
              │         Kafka Cluster                     │
              │  ┌─────────┐  ┌─────────┐  ┌─────────┐   │
              │  │Broker-1 │  │Broker-2 │  │Broker-3 │   │
              │  │Leader   │  │Replica  │  │Replica  │   │
              │  └─────────┘  └─────────┘  └─────────┘   │
              └───────────────────────────────────────────┘
                                      │
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Consumer Applications                                  │
├─────────────────┬─────────────────┬─────────────────┬─────────────────────┤
│  Strategy       │  Risk           │  Portfolio      │  Notification       │
│  Engine         │  Management     │  Tracker        │  Service            │
├─────────────────┼─────────────────┼─────────────────┼─────────────────────┤
│ • Market data   │ • Order events  │ • Position      │ • Alert events      │
│ • Price signals │ • Risk metrics  │ • P&L events    │ • System status     │
│ • Indicators    │ • Limit checks  │ • Performance   │ • User notifications│
└─────────────────┴─────────────────┴─────────────────┴─────────────────────┘
```

### Key Components

#### 1. Kafka Cluster Configuration

**High-Performance Setup:**
- 3-broker Kafka cluster (kafka1, kafka2, kafka3) for fault tolerance
- Initial deployment with local storage, migration path to 4TB NVMe SSD
- XFS filesystem optimization on dedicated SSD volume (`/mnt/ssd4tb/kafka/`)
- 8 I/O threads and 8 network threads per broker for optimal throughput
- LZ4 compression for ~40% reduction in network and storage usage
- 1GB log segments with 7-day retention for regulatory compliance
- JVM heap optimization: 8GB per broker with G1GC for low-latency
- Confluent Schema Registry for schema management
- Kafka UI for visual cluster monitoring

```yaml
# infrastructure/kafka/docker-compose.kafka.yml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: jts-zookeeper
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CONNECTIONS: 60
    volumes:
      - /var/lib/kafka/zookeeper:/var/lib/zookeeper/data
      - /var/lib/kafka/zookeeper-log:/var/lib/zookeeper/log
    ulimits:
      nofile: 65536

  kafka1:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka1
    container_name: jts-kafka1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "19092:19092"
    volumes:
      # Initial: ./data/kafka/kafka1:/var/lib/kafka/data
      # Production: /mnt/ssd4tb/kafka/kafka1:/var/lib/kafka/data
      - ./data/kafka/kafka1:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      
      # Performance Optimization
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      
      # Log Configuration
      KAFKA_LOG_RETENTION_HOURS: 168  # 7 days
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      KAFKA_COMPRESSION_TYPE: "lz4"
      
      # JVM Settings (production: increase to "-Xmx8G -Xms8G")
      KAFKA_HEAP_OPTS: "-Xmx6G -Xms6G"
      KAFKA_JVM_PERFORMANCE_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35"
    ulimits:
      nofile: 65536

  kafka2:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka2
    container_name: jts-kafka2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
      - "19093:19093"
    volumes:
      - ./data/kafka/kafka2:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:29093,PLAINTEXT_HOST://localhost:9093
      # ... (same settings as kafka1)
    ulimits:
      nofile: 65536

  kafka3:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka3
    container_name: jts-kafka3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
      - "19094:19094"
    volumes:
      - ./data/kafka/kafka3:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:29094,PLAINTEXT_HOST://localhost:9094
      # ... (same settings as kafka1)
    ulimits:
      nofile: 65536

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: jts-schema-registry
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka1:29092,kafka2:29093,kafka3:29094'
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 2
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: "backward"
    networks:
      - jts-network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: jts-kafka-ui
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - schema-registry
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: jts-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:29092,kafka2:29093,kafka3:29094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    networks:
      - jts-network

networks:
  jts-network:
    driver: bridge
```

#### 2. Topic Design and Partitioning Strategy

**Strategic Topic Architecture:**

```bash
#!/bin/bash
# infrastructure/kafka/setup-topics.sh

set -e

echo "Creating JTS trading system topics..."

# Market Data Topics - High throughput with strategic partitioning
# Korean stock market data (KRX)
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic market-data.krx.ticks \
  --partitions 12 \
  --replication-factor 2 \
  --config retention.ms=86400000 \
  --config segment.ms=3600000 \
  --config compression.type=lz4 \
  --config max.message.bytes=1000000

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic market-data.krx.candles \
  --partitions 6 \
  --replication-factor 2 \
  --config retention.ms=604800000 \
  --config segment.ms=86400000 \
  --config compression.type=lz4

# Cryptocurrency market data
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic market-data.crypto.ticks \
  --partitions 12 \
  --replication-factor 2 \
  --config retention.ms=86400000 \
  --config segment.ms=3600000 \
  --config compression.type=lz4

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic market-data.crypto.candles \
  --partitions 6 \
  --replication-factor 2 \
  --config retention.ms=604800000 \
  --config segment.ms=86400000 \
  --config compression.type=lz4

# Trading signal and order topics
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic signals.entry.buy \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=2592000000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic signals.entry.sell \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=2592000000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic orders.pending \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=86400000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic orders.executions \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=2592000000 \
  --config compression.type=snappy \
  --config min.insync.replicas=2

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic orders.failures \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=604800000 \
  --config compression.type=snappy

# Portfolio and risk topics
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic portfolio.updates \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=2592000000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic risk.alerts \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=604800000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic risk.metrics \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=86400000 \
  --config compression.type=snappy

# System monitoring topics
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic system.health \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=259200000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic system.errors \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=604800000 \
  --config compression.type=snappy

# Backtesting topics
docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic backtest.requests \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=86400000 \
  --config compression.type=snappy

docker exec jts-kafka1 kafka-topics --create \
  --bootstrap-server kafka1:29092 \
  --topic backtest.results \
  --partitions 3 \
  --replication-factor 2 \
  --config retention.ms=2592000000 \
  --config compression.type=snappy

echo "All topics created successfully!"

# Display topic list
kafka-topics --bootstrap-server $KAFKA_BROKERS --list
```

#### 3. Protocol Buffers Message Schema

**Core Message Definitions:**

```protobuf
// schemas/protobuf/market_data.proto
syntax = "proto3";

package jts.market;

import "google/protobuf/timestamp.proto";
import "common.proto";

// Real-time price tick data
message PriceTick {
  string symbol = 1;
  string exchange = 2;
  google.protobuf.Timestamp timestamp = 3;
  jts.common.Decimal price = 4;
  jts.common.Decimal volume = 5;
  optional jts.common.Decimal bid_price = 6;
  optional jts.common.Decimal ask_price = 7;
  optional jts.common.Decimal bid_size = 8;
  optional jts.common.Decimal ask_size = 9;
  uint32 trade_count = 10;
  optional jts.common.Decimal vwap = 11;
}

// Order book snapshot
message OrderBookSnapshot {
  string symbol = 1;
  string exchange = 2;
  google.protobuf.Timestamp timestamp = 3;
  repeated PriceLevel bids = 4;
  repeated PriceLevel asks = 5;
  jts.common.Decimal spread = 6;
  jts.common.Decimal mid_price = 7;
}

message PriceLevel {
  jts.common.Decimal price = 1;
  jts.common.Decimal size = 2;
}

// OHLCV candle data
message OHLCVCandle {
  string symbol = 1;
  string exchange = 2;
  google.protobuf.Timestamp timestamp = 3;
  jts.common.Decimal open = 4;
  jts.common.Decimal high = 5;
  jts.common.Decimal low = 6;
  jts.common.Decimal close = 7;
  jts.common.Decimal volume = 8;
  uint32 trade_count = 9;
  jts.common.Decimal vwap = 10;
  string timeframe = 11; // 1m, 5m, 15m, 1h, 4h, 1d
}
```

```protobuf
// schemas/protobuf/trading.proto
syntax = "proto3";

package jts.trading;

import "google/protobuf/timestamp.proto";
import "common.proto";

// Order lifecycle events
message OrderEvent {
  string order_id = 1;
  string strategy_id = 2;
  string user_id = 3;
  string symbol = 4;
  string exchange = 5;
  OrderSide side = 6;
  OrderType type = 7;
  OrderStatus status = 8;
  jts.common.Decimal quantity = 9;
  optional jts.common.Decimal price = 10;
  optional jts.common.Decimal stop_price = 11;
  jts.common.Decimal filled_quantity = 12;
  optional jts.common.Decimal average_price = 13;
  jts.common.Decimal commission = 14;
  string commission_currency = 15;
  TimeInForce time_in_force = 16;
  google.protobuf.Timestamp created_at = 17;
  google.protobuf.Timestamp updated_at = 18;
  optional string error_message = 19;
  map<string, string> metadata = 20;
}

enum OrderSide {
  ORDER_SIDE_UNKNOWN = 0;
  ORDER_SIDE_BUY = 1;
  ORDER_SIDE_SELL = 2;
}

enum OrderType {
  ORDER_TYPE_UNKNOWN = 0;
  ORDER_TYPE_MARKET = 1;
  ORDER_TYPE_LIMIT = 2;
  ORDER_TYPE_STOP = 3;
  ORDER_TYPE_STOP_LIMIT = 4;
}

enum OrderStatus {
  ORDER_STATUS_UNKNOWN = 0;
  ORDER_STATUS_PENDING = 1;
  ORDER_STATUS_SUBMITTED = 2;
  ORDER_STATUS_ACCEPTED = 3;
  ORDER_STATUS_PARTIAL = 4;
  ORDER_STATUS_FILLED = 5;
  ORDER_STATUS_CANCELLED = 6;
  ORDER_STATUS_REJECTED = 7;
  ORDER_STATUS_EXPIRED = 8;
}

enum TimeInForce {
  TIME_IN_FORCE_UNKNOWN = 0;
  TIME_IN_FORCE_GTC = 1; // Good Till Cancelled
  TIME_IN_FORCE_IOC = 2; // Immediate or Cancel
  TIME_IN_FORCE_FOK = 3; // Fill or Kill
  TIME_IN_FORCE_DAY = 4; // Day Order
}
```

```protobuf
// schemas/protobuf/strategy.proto
syntax = "proto3";

package jts.strategy;

import "google/protobuf/timestamp.proto";
import "common.proto";

// Trading signal generated by strategy
message TradingSignal {
  string signal_id = 1;
  string strategy_id = 2;
  string symbol = 3;
  string exchange = 4;
  SignalType type = 5;
  SignalDirection direction = 6;
  jts.common.Decimal confidence = 7;
  jts.common.Decimal suggested_quantity = 8;
  optional jts.common.Decimal suggested_price = 9;
  google.protobuf.Timestamp generated_at = 10;
  google.protobuf.Timestamp expires_at = 11;
  map<string, string> indicators = 12;
  string reason = 13;
}

enum SignalType {
  SIGNAL_TYPE_UNKNOWN = 0;
  SIGNAL_TYPE_ENTRY = 1;
  SIGNAL_TYPE_EXIT = 2;
  SIGNAL_TYPE_STOP_LOSS = 3;
  SIGNAL_TYPE_TAKE_PROFIT = 4;
  SIGNAL_TYPE_SCALE_IN = 5;
  SIGNAL_TYPE_SCALE_OUT = 6;
}

enum SignalDirection {
  SIGNAL_DIRECTION_UNKNOWN = 0;
  SIGNAL_DIRECTION_LONG = 1;
  SIGNAL_DIRECTION_SHORT = 2;
  SIGNAL_DIRECTION_CLOSE = 3;
}

// Strategy performance metrics
message PerformanceMetrics {
  string strategy_id = 1;
  google.protobuf.Timestamp period_start = 2;
  google.protobuf.Timestamp period_end = 3;
  jts.common.Decimal total_pnl = 4;
  jts.common.Decimal realized_pnl = 5;
  jts.common.Decimal unrealized_pnl = 6;
  jts.common.Decimal total_commission = 7;
  uint32 total_trades = 8;
  uint32 winning_trades = 9;
  uint32 losing_trades = 10;
  jts.common.Decimal win_rate = 11;
  jts.common.Decimal max_drawdown = 12;
  jts.common.Decimal sharpe_ratio = 13;
  jts.common.Decimal sortino_ratio = 14;
  jts.common.Decimal volatility = 15;
}
```

```protobuf
// schemas/protobuf/common.proto
syntax = "proto3";

package jts.common;

// High precision decimal for financial calculations
message Decimal {
  string value = 1; // String representation to avoid precision loss
  int32 scale = 2;  // Number of decimal places
}

// Common error information
message ErrorInfo {
  string code = 1;
  string message = 2;
  string details = 3;
  int64 timestamp = 4;
}

// Health check status
message HealthStatus {
  string service_name = 1;
  HealthState state = 2;
  string message = 3;
  int64 timestamp = 4;
  map<string, string> metrics = 5;
}

enum HealthState {
  HEALTH_STATE_UNKNOWN = 0;
  HEALTH_STATE_HEALTHY = 1;
  HEALTH_STATE_DEGRADED = 2;
  HEALTH_STATE_UNHEALTHY = 3;
}
```

#### 4. Producer Pattern Implementation

**High-Performance Message Producer:**

```typescript
// libs/shared/messaging/src/lib/kafka-producer.service.ts
import { Injectable, Logger, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { Kafka, Producer, ProducerRecord, RecordMetadata } from 'kafkajs';
import { ConfigService } from '@nestjs/config';
import { setTimeout } from 'timers/promises';

@Injectable()
export class KafkaProducerService implements OnModuleInit, OnModuleDestroy {
  private readonly logger = new Logger(KafkaProducerService.name);
  private kafka: Kafka;
  private producer: Producer;
  private isConnected = false;

  constructor(private configService: ConfigService) {
    this.kafka = new Kafka({
      clientId: 'jts-producer',
      brokers: this.getBrokers(),
      retry: {
        retries: 5,
        initialRetryTime: 100,
        maxRetryTime: 30000,
        multiplier: 2,
      },
      connectionTimeout: 10000,
      requestTimeout: 30000,
    });

    this.producer = this.kafka.producer({
      // Optimize for throughput with batching
      maxInFlightRequests: 5,
      idempotent: true, // Exactly-once semantics
      transactionTimeout: 30000,
      
      // Batching configuration
      batchSize: 16384, // 16KB batches
      lingerMs: 10, // Wait up to 10ms to batch messages
      
      // Compression
      compression: 'lz4',
      
      // Retry configuration
      retry: {
        maxRetryTime: 30000,
        initialRetryTime: 100,
        retries: 5,
      },
    });
  }

  async onModuleInit() {
    await this.connect();
  }

  async onModuleDestroy() {
    await this.disconnect();
  }

  private async connect(): Promise<void> {
    try {
      await this.producer.connect();
      this.isConnected = true;
      this.logger.log('Kafka producer connected successfully');
    } catch (error) {
      this.logger.error('Failed to connect Kafka producer', error);
      throw error;
    }
  }

  private async disconnect(): Promise<void> {
    try {
      await this.producer.disconnect();
      this.isConnected = false;
      this.logger.log('Kafka producer disconnected');
    } catch (error) {
      this.logger.error('Error disconnecting Kafka producer', error);
    }
  }

  private getBrokers(): string[] {
    const brokers = this.configService.get<string>('KAFKA_BROKERS', 'localhost:9092');
    return brokers.split(',').map(broker => broker.trim());
  }

  /**
   * Send a single message with automatic retry and error handling
   */
  async sendMessage(
    topic: string,
    message: any,
    key?: string,
    partition?: number,
    headers?: Record<string, string>
  ): Promise<RecordMetadata[]> {
    if (!this.isConnected) {
      await this.connect();
    }

    const record: ProducerRecord = {
      topic,
      messages: [
        {
          key,
          value: Buffer.from(JSON.stringify(message)),
          partition,
          headers: {
            ...headers,
            timestamp: Date.now().toString(),
            producer: 'jts-trading-system',
          },
        },
      ],
    };

    return this.sendWithRetry(record);
  }

  /**
   * Send multiple messages in a batch for optimal throughput
   */
  async sendBatch(
    topic: string,
    messages: Array<{
      key?: string;
      value: any;
      partition?: number;
      headers?: Record<string, string>;
    }>
  ): Promise<RecordMetadata[]> {
    if (!this.isConnected) {
      await this.connect();
    }

    const record: ProducerRecord = {
      topic,
      messages: messages.map(msg => ({
        key: msg.key,
        value: Buffer.from(JSON.stringify(msg.value)),
        partition: msg.partition,
        headers: {
          ...msg.headers,
          timestamp: Date.now().toString(),
          producer: 'jts-trading-system',
        },
      })),
    };

    return this.sendWithRetry(record);
  }

  /**
   * Send Protocol Buffers encoded message
   */
  async sendProtobufMessage(
    topic: string,
    messageBuffer: Buffer,
    key?: string,
    partition?: number,
    headers?: Record<string, string>
  ): Promise<RecordMetadata[]> {
    if (!this.isConnected) {
      await this.connect();
    }

    const record: ProducerRecord = {
      topic,
      messages: [
        {
          key,
          value: messageBuffer,
          partition,
          headers: {
            ...headers,
            timestamp: Date.now().toString(),
            producer: 'jts-trading-system',
            encoding: 'protobuf',
          },
        },
      ],
    };

    return this.sendWithRetry(record);
  }

  private async sendWithRetry(
    record: ProducerRecord,
    maxRetries = 3
  ): Promise<RecordMetadata[]> {
    let lastError: Error;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const result = await this.producer.send(record);
        
        this.logger.debug(
          `Message sent to topic ${record.topic}: ${result.length} messages`
        );
        
        return result;
      } catch (error) {
        lastError = error as Error;
        this.logger.warn(
          `Failed to send message to ${record.topic} (attempt ${attempt}/${maxRetries})`,
          error
        );

        if (attempt < maxRetries) {
          // Exponential backoff
          const delay = Math.min(1000 * Math.pow(2, attempt - 1), 10000);
          await setTimeout(delay);
        }
      }
    }

    this.logger.error(
      `Failed to send message to ${record.topic} after ${maxRetries} attempts`,
      lastError
    );
    throw lastError;
  }

  /**
   * Health check for producer
   */
  async isHealthy(): Promise<boolean> {
    try {
      // Send a small test message to a health check topic
      await this.producer.send({
        topic: 'system.health.status',
        messages: [
          {
            value: JSON.stringify({
              service: 'kafka-producer',
              timestamp: Date.now(),
              status: 'healthy',
            }),
          },
        ],
      });
      return true;
    } catch (error) {
      this.logger.error('Producer health check failed', error);
      return false;
    }
  }
}
```

#### 5. Consumer Pattern Implementation

**Robust Message Consumer with Error Handling:**

```typescript
// libs/shared/messaging/src/lib/kafka-consumer.service.ts
import { Injectable, Logger, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { 
  Kafka, 
  Consumer, 
  ConsumerRunConfig, 
  EachMessagePayload,
  EachBatchPayload 
} from 'kafkajs';
import { ConfigService } from '@nestjs/config';

export interface MessageHandler {
  (message: EachMessagePayload): Promise<void>;
}

export interface BatchHandler {
  (batch: EachBatchPayload): Promise<void>;
}

@Injectable()
export class KafkaConsumerService implements OnModuleInit, OnModuleDestroy {
  private readonly logger = new Logger(KafkaConsumerService.name);
  private kafka: Kafka;
  private consumers: Map<string, Consumer> = new Map();

  constructor(private configService: ConfigService) {
    this.kafka = new Kafka({
      clientId: 'jts-consumer',
      brokers: this.getBrokers(),
      retry: {
        retries: 5,
        initialRetryTime: 100,
        maxRetryTime: 30000,
        multiplier: 2,
      },
    });
  }

  async onModuleInit() {
    // Consumers are created and started when subscriptions are made
    this.logger.log('Kafka consumer service initialized');
  }

  async onModuleDestroy() {
    await this.disconnectAll();
  }

  private getBrokers(): string[] {
    const brokers = this.configService.get<string>('KAFKA_BROKERS', 'localhost:9092');
    return brokers.split(',').map(broker => broker.trim());
  }

  /**
   * Subscribe to a topic with message-by-message processing
   */
  async subscribe(
    topics: string[],
    groupId: string,
    handler: MessageHandler,
    options?: {
      fromBeginning?: boolean;
      sessionTimeout?: number;
      heartbeatInterval?: number;
      maxBytesPerPartition?: number;
      maxBytes?: number;
    }
  ): Promise<void> {
    const consumer = this.kafka.consumer({
      groupId,
      sessionTimeout: options?.sessionTimeout || 30000,
      heartbeatInterval: options?.heartbeatInterval || 3000,
      maxBytesPerPartition: options?.maxBytesPerPartition || 1048576, // 1MB
      maxBytes: options?.maxBytes || 10485760, // 10MB
      retry: {
        retries: 3,
        initialRetryTime: 100,
      },
    });

    await consumer.connect();
    await consumer.subscribe({ 
      topics,
      fromBeginning: options?.fromBeginning || false 
    });

    const runConfig: ConsumerRunConfig = {
      eachMessage: async (payload) => {
        const { topic, partition, message } = payload;
        
        try {
          this.logger.debug(
            `Processing message from ${topic}[${partition}] offset ${message.offset}`
          );
          
          const startTime = Date.now();
          await handler(payload);
          const duration = Date.now() - startTime;
          
          this.logger.debug(
            `Message processed in ${duration}ms from ${topic}[${partition}]`
          );
        } catch (error) {
          this.logger.error(
            `Error processing message from ${topic}[${partition}] offset ${message.offset}`,
            error
          );
          
          await this.handleFailedMessage(payload, error as Error);
        }
      },
    };

    await consumer.run(runConfig);
    this.consumers.set(groupId, consumer);
    
    this.logger.log(`Subscribed to topics: ${topics.join(', ')} with group: ${groupId}`);
  }

  /**
   * Subscribe to a topic with batch processing for high throughput
   */
  async subscribeBatch(
    topics: string[],
    groupId: string,
    batchHandler: BatchHandler,
    options?: {
      fromBeginning?: boolean;
      batchSize?: number;
      maxWait?: number;
    }
  ): Promise<void> {
    const consumer = this.kafka.consumer({
      groupId,
      maxBytesPerPartition: 1048576 * (options?.batchSize || 100), // Scale with batch size
      sessionTimeout: 30000,
      heartbeatInterval: 3000,
    });

    await consumer.connect();
    await consumer.subscribe({ 
      topics,
      fromBeginning: options?.fromBeginning || false 
    });

    const runConfig: ConsumerRunConfig = {
      eachBatch: async (batchPayload) => {
        const { batch } = batchPayload;
        
        try {
          this.logger.debug(
            `Processing batch of ${batch.messages.length} messages from ${batch.topic}[${batch.partition}]`
          );
          
          const startTime = Date.now();
          await batchHandler(batchPayload);
          const duration = Date.now() - startTime;
          
          this.logger.debug(
            `Batch processed in ${duration}ms from ${batch.topic}[${batch.partition}]`
          );
        } catch (error) {
          this.logger.error(
            `Error processing batch from ${batch.topic}[${batch.partition}]`,
            error
          );
          
          // Handle batch failure - could implement partial retry logic here
          throw error;
        }
      },
    };

    await consumer.run(runConfig);
    this.consumers.set(groupId, consumer);
    
    this.logger.log(`Subscribed to batch topics: ${topics.join(', ')} with group: ${groupId}`);
  }

  /**
   * Handle failed message processing - send to DLQ if configured
   */
  private async handleFailedMessage(
    payload: EachMessagePayload,
    error: Error
  ): Promise<void> {
    const { topic, partition, message } = payload;
    const dlqTopic = `dlq.${topic.replace(/\./g, '.')}`;
    
    try {
      // Create a simple producer for DLQ (could be injected instead)
      const producer = this.kafka.producer();
      await producer.connect();
      
      await producer.send({
        topic: dlqTopic,
        messages: [
          {
            key: message.key,
            value: message.value,
            headers: {
              ...message.headers,
              'original-topic': topic,
              'original-partition': partition.toString(),
              'original-offset': message.offset,
              'error-message': error.message,
              'error-timestamp': Date.now().toString(),
            },
          },
        ],
      });
      
      await producer.disconnect();
      
      this.logger.warn(
        `Message sent to DLQ ${dlqTopic} from ${topic}[${partition}] offset ${message.offset}`
      );
    } catch (dlqError) {
      this.logger.error(
        `Failed to send message to DLQ ${dlqTopic}`,
        dlqError
      );
    }
  }

  /**
   * Disconnect a specific consumer
   */
  async disconnect(groupId: string): Promise<void> {
    const consumer = this.consumers.get(groupId);
    if (consumer) {
      await consumer.disconnect();
      this.consumers.delete(groupId);
      this.logger.log(`Disconnected consumer for group: ${groupId}`);
    }
  }

  /**
   * Disconnect all consumers
   */
  private async disconnectAll(): Promise<void> {
    const disconnectPromises = Array.from(this.consumers.entries()).map(
      async ([groupId, consumer]) => {
        try {
          await consumer.disconnect();
          this.logger.log(`Disconnected consumer for group: ${groupId}`);
        } catch (error) {
          this.logger.error(`Error disconnecting consumer for group: ${groupId}`, error);
        }
      }
    );

    await Promise.allSettled(disconnectPromises);
    this.consumers.clear();
  }

  /**
   * Get consumer lag information for monitoring
   */
  async getConsumerLag(groupId: string): Promise<any> {
    try {
      const admin = this.kafka.admin();
      await admin.connect();
      
      const offsets = await admin.fetchOffsets({ groupId });
      await admin.disconnect();
      
      return offsets;
    } catch (error) {
      this.logger.error(`Failed to fetch consumer lag for group: ${groupId}`, error);
      throw error;
    }
  }
}
```

#### 6. Event Streaming Architecture Implementation

**Stream Processing Service:**

```typescript
// libs/shared/messaging/src/lib/stream-processor.service.ts
import { Injectable, Logger } from '@nestjs/common';
import { KafkaConsumerService, MessageHandler } from './kafka-consumer.service';
import { KafkaProducerService } from './kafka-producer.service';
import { EachMessagePayload } from 'kafkajs';

@Injectable()
export class StreamProcessorService {
  private readonly logger = new Logger(StreamProcessorService.name);

  constructor(
    private consumerService: KafkaConsumerService,
    private producerService: KafkaProducerService
  ) {}

  /**
   * Set up market data aggregation stream
   */
  async setupMarketDataAggregation(): Promise<void> {
    const handler: MessageHandler = async (payload: EachMessagePayload) => {
      const { topic, message } = payload;
      
      if (topic === 'market.price.ticks') {
        const priceTick = JSON.parse(message.value?.toString() || '{}');
        
        // Aggregate to OHLCV candles
        const ohlcvCandle = await this.aggregateToOHLCV(priceTick);
        
        if (ohlcvCandle) {
          await this.producerService.sendMessage(
            'market.ohlcv.aggregated',
            ohlcvCandle,
            `${priceTick.symbol}-${priceTick.exchange}`
          );
        }
        
        // Update order book if needed
        if (priceTick.bid_price && priceTick.ask_price) {
          await this.updateOrderBookSnapshot(priceTick);
        }
      }
    };

    await this.consumerService.subscribe(
      ['market.price.ticks'],
      'market-data-aggregator',
      handler
    );
  }

  /**
   * Set up order lifecycle tracking stream
   */
  async setupOrderLifecycleTracking(): Promise<void> {
    const handler: MessageHandler = async (payload: EachMessagePayload) => {
      const { topic, message } = payload;
      const orderEvent = JSON.parse(message.value?.toString() || '{}');
      
      // Update portfolio positions based on order execution
      if (topic === 'trading.orders.executed') {
        await this.updatePortfolioPosition(orderEvent);
        
        // Calculate P&L
        const pnlUpdate = await this.calculatePnL(orderEvent);
        if (pnlUpdate) {
          await this.producerService.sendMessage(
            'portfolio.pnl.calculated',
            pnlUpdate,
            orderEvent.strategy_id
          );
        }
      }
      
      // Check risk limits
      if (['trading.orders.submitted', 'trading.orders.executed'].includes(topic)) {
        const riskCheck = await this.checkRiskLimits(orderEvent);
        if (riskCheck.violation) {
          await this.producerService.sendMessage(
            'risk.violations.detected',
            riskCheck,
            orderEvent.strategy_id
          );
        }
      }
    };

    await this.consumerService.subscribe(
      [
        'trading.orders.submitted',
        'trading.orders.executed',
        'trading.orders.cancelled',
        'trading.orders.rejected'
      ],
      'order-lifecycle-tracker',
      handler
    );
  }

  /**
   * Set up strategy performance calculation stream
   */
  async setupStrategyPerformanceCalculation(): Promise<void> {
    const handler: MessageHandler = async (payload: EachMessagePayload) => {
      const { topic, message } = payload;
      
      if (topic === 'portfolio.pnl.calculated') {
        const pnlEvent = JSON.parse(message.value?.toString() || '{}');
        
        // Calculate strategy performance metrics
        const performanceMetrics = await this.calculateStrategyPerformance(
          pnlEvent.strategy_id
        );
        
        await this.producerService.sendMessage(
          'strategy.performance.metrics',
          performanceMetrics,
          pnlEvent.strategy_id
        );
      }
    };

    await this.consumerService.subscribe(
      ['portfolio.pnl.calculated'],
      'strategy-performance-calculator',
      handler
    );
  }

  private async aggregateToOHLCV(priceTick: any): Promise<any | null> {
    // Implementation for OHLCV aggregation
    // This would typically use a time window and state management
    
    this.logger.debug(`Aggregating price tick for ${priceTick.symbol}`);
    
    // Simplified example - real implementation would use proper time windows
    return {
      symbol: priceTick.symbol,
      exchange: priceTick.exchange,
      timestamp: new Date().toISOString(),
      open: priceTick.price,
      high: priceTick.price,
      low: priceTick.price,
      close: priceTick.price,
      volume: priceTick.volume,
      trade_count: 1,
      vwap: priceTick.price,
      timeframe: '1m'
    };
  }

  private async updateOrderBookSnapshot(priceTick: any): Promise<void> {
    // Update order book snapshot logic
    this.logger.debug(`Updating order book for ${priceTick.symbol}`);
  }

  private async updatePortfolioPosition(orderEvent: any): Promise<void> {
    // Update portfolio position based on order execution
    this.logger.debug(`Updating portfolio position for order ${orderEvent.order_id}`);
    
    const positionUpdate = {
      user_id: orderEvent.user_id,
      strategy_id: orderEvent.strategy_id,
      symbol: orderEvent.symbol,
      exchange: orderEvent.exchange,
      quantity_change: orderEvent.side === 'BUY' ? orderEvent.filled_quantity : -orderEvent.filled_quantity,
      average_price: orderEvent.average_price,
      commission: orderEvent.commission,
      timestamp: new Date().toISOString()
    };
    
    await this.producerService.sendMessage(
      'portfolio.positions.updated',
      positionUpdate,
      `${orderEvent.user_id}-${orderEvent.symbol}-${orderEvent.exchange}`
    );
  }

  private async calculatePnL(orderEvent: any): Promise<any | null> {
    // Calculate P&L based on order execution
    this.logger.debug(`Calculating P&L for order ${orderEvent.order_id}`);
    
    // Simplified P&L calculation
    return {
      strategy_id: orderEvent.strategy_id,
      symbol: orderEvent.symbol,
      realized_pnl: 0, // Would calculate based on position history
      unrealized_pnl: 0, // Would calculate based on current market price
      commission: orderEvent.commission,
      timestamp: new Date().toISOString()
    };
  }

  private async checkRiskLimits(orderEvent: any): Promise<any> {
    // Check risk limits for the order
    this.logger.debug(`Checking risk limits for order ${orderEvent.order_id}`);
    
    // Simplified risk check - real implementation would check various limits
    return {
      violation: false,
      strategy_id: orderEvent.strategy_id,
      order_id: orderEvent.order_id,
      checks_performed: ['position_size', 'daily_loss', 'exposure'],
      timestamp: new Date().toISOString()
    };
  }

  private async calculateStrategyPerformance(strategyId: string): Promise<any> {
    // Calculate comprehensive strategy performance metrics
    this.logger.debug(`Calculating performance metrics for strategy ${strategyId}`);
    
    return {
      strategy_id: strategyId,
      period_start: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),
      period_end: new Date().toISOString(),
      total_pnl: '0.00',
      realized_pnl: '0.00',
      unrealized_pnl: '0.00',
      total_commission: '0.00',
      total_trades: 0,
      winning_trades: 0,
      losing_trades: 0,
      win_rate: '0.00',
      max_drawdown: '0.00',
      sharpe_ratio: '0.00',
      sortino_ratio: '0.00',
      volatility: '0.00'
    };
  }
}
```

### Storage Migration Strategy

#### Initial Setup (Without 4TB SSD)
```bash
# Create data directories
cd infrastructure/kafka
mkdir -p data/zookeeper/data data/zookeeper/logs
mkdir -p data/kafka/kafka1 data/kafka/kafka2 data/kafka/kafka3

# Start Kafka cluster
docker-compose -f docker-compose.kafka.yml up -d

# Verify cluster health
docker-compose -f docker-compose.kafka.yml ps
docker exec jts-kafka1 kafka-broker-api-versions --bootstrap-server kafka1:29092
```

#### After Installing 4TB SSD
```bash
# Stop Kafka cluster
docker-compose -f docker-compose.kafka.yml down

# Mount the SSD (Samsung 990 PRO or similar)
sudo fdisk -l
sudo parted /dev/nvme1n1 mklabel gpt
sudo parted /dev/nvme1n1 mkpart primary ext4 0% 100%
sudo mkfs.xfs /dev/nvme1n1p1

# Create mount point and configure
sudo mkdir -p /mnt/ssd4tb
echo "UUID=$(sudo blkid -s UUID -o value /dev/nvme1n1p1) /mnt/ssd4tb xfs defaults,noatime,nobarrier,logbufs=8 0 2" | sudo tee -a /etc/fstab
sudo mount -a

# Set permissions and create directories
sudo chown -R $USER:$USER /mnt/ssd4tb
mkdir -p /mnt/ssd4tb/kafka/{kafka1,kafka2,kafka3}
mkdir -p /mnt/ssd4tb/zookeeper/{data,logs}

# Move existing data
cp -r data/kafka/* /mnt/ssd4tb/kafka/
cp -r data/zookeeper/* /mnt/ssd4tb/zookeeper/

# Update docker-compose.kafka.yml volumes to use /mnt/ssd4tb paths
# Restart cluster
docker-compose -f docker-compose.kafka.yml up -d
```

### System Performance Optimization

#### Linux Kernel Tuning
```bash
# Add to /etc/sysctl.conf for optimal Kafka performance
cat << 'EOF' | sudo tee -a /etc/sysctl.conf
# Network optimizations for Kafka
net.core.rmem_default=31457280
net.core.rmem_max=33554432
net.core.wmem_default=31457280
net.core.wmem_max=33554432
net.core.netdev_max_backlog=5000
net.ipv4.tcp_rmem=4096 87380 33554432
net.ipv4.tcp_wmem=4096 65536 33554432
net.ipv4.tcp_max_syn_backlog=8096
net.ipv4.tcp_slow_start_after_idle=0
net.ipv4.tcp_tw_reuse=1
net.ipv4.ip_local_port_range=10240 65535

# File system optimizations
vm.swappiness=1
vm.dirty_background_ratio=5
vm.dirty_ratio=15
EOF

# Apply settings
sudo sysctl -p
```

### Implementation Steps

#### Phase 1: Infrastructure Setup (Days 1-3)
1. **Initial Deployment**
   ```bash
   # Deploy Kafka cluster with local storage
   cd infrastructure/kafka
   docker-compose -f docker-compose.kafka.yml up -d
   
   # Verify cluster health
   docker logs jts-kafka1
   docker logs jts-kafka2
   docker logs jts-kafka3
   ```

2. **Topic Creation and Configuration**
   ```bash
   # Create all topics with proper partitioning
   chmod +x setup-topics.sh
   ./setup-topics.sh
   
   # Access Kafka UI at http://localhost:8080
   ```

#### Phase 2: Schema and Serialization (Days 4-5)
1. **Protocol Buffers Setup**
   ```bash
   # Install protobuf compiler
   npm install -g protobuf
   npm install @types/google-protobuf google-protobuf
   
   # Generate TypeScript definitions
   mkdir -p libs/shared/messaging/src/generated
   protoc --ts_out=libs/shared/messaging/src/generated schemas/protobuf/*.proto
   ```

2. **Schema Registry Configuration**
   ```bash
   # Register all schemas
   curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
     --data @schemas/protobuf/market_data.proto \
     http://localhost:8081/subjects/market.price.ticks-value/versions
   ```

#### Phase 3: Producer/Consumer Implementation (Days 6-8)
1. **Producer Service Integration**
   ```typescript
   // Add to each microservice that produces messages
   import { KafkaProducerService } from '@jts/shared/messaging';
   
   @Module({
     imports: [SharedMessagingModule],
     // ...
   })
   export class MarketDataModule {}
   ```

2. **Consumer Service Setup**
   ```typescript
   // Set up consumers in each service
   async onModuleInit() {
     await this.streamProcessor.setupMarketDataAggregation();
     await this.streamProcessor.setupOrderLifecycleTracking();
     await this.streamProcessor.setupStrategyPerformanceCalculation();
   }
   ```

#### Phase 4: Monitoring and Testing (Days 9-10)
1. **Monitoring Setup**
   ```yaml
   # Add Prometheus Kafka exporter to docker-compose
   kafka-exporter:
     image: danielqsj/kafka-exporter:latest
     container_name: jts-kafka-exporter
     ports:
       - "9308:9308"
     command:
       - '--kafka.server=kafka1:29092'
       - '--kafka.server=kafka2:29093'
       - '--kafka.server=kafka3:29094'
     networks:
       - jts-network
   ```

2. **Performance Testing**
   ```bash
   # Test producer performance
   docker exec jts-kafka1 kafka-producer-perf-test \
     --topic market-data.krx.ticks \
     --num-records 100000 --record-size 1000 \
     --throughput 10000 --producer-props bootstrap.servers=kafka1:29092
   
   # Test consumer performance
   docker exec jts-kafka1 kafka-consumer-perf-test \
     --topic market-data.krx.ticks \
     --bootstrap-server kafka1:29092 --messages 100000
   ```

### Backup and Recovery Strategy

#### Daily Backup Script
```bash
#!/bin/bash
# infrastructure/kafka/backup-topics.sh

BACKUP_DIR="/mnt/nas/kafka-backups/$(date +%Y%m%d)"
mkdir -p $BACKUP_DIR

# Export critical topics
for topic in orders.executions portfolio.updates risk.alerts; do
  docker exec jts-kafka1 kafka-console-consumer \
    --bootstrap-server kafka1:29092 \
    --topic $topic \
    --from-beginning \
    --property print.timestamp=true \
    --timeout-ms 10000 > $BACKUP_DIR/$topic.json
done

# Sync to NAS for redundancy
rsync -av /mnt/ssd4tb/kafka/ /mnt/nas/kafka-mirror/
```

### Troubleshooting Guide

#### Common Issues
```bash
# Check broker availability
docker logs jts-kafka1
docker-compose -f docker-compose.kafka.yml restart kafka1

# Check consumer lag
docker exec jts-kafka1 kafka-consumer-groups \
  --bootstrap-server kafka1:29092 --list

docker exec jts-kafka1 kafka-consumer-groups \
  --bootstrap-server kafka1:29092 \
  --describe --group <group-name>

# Force log cleanup for disk space
docker exec jts-kafka1 kafka-configs \
  --bootstrap-server kafka1:29092 \
  --alter --entity-type topics \
  --entity-name <topic-name> \
  --add-config retention.ms=3600000
```

## Dependencies

- **1001**: Storage Infrastructure - Required for Kafka LVM volume setup

## Testing Plan

### Functional Testing
- **Topic Creation**: Verify all topics are created with correct partitions and replication
- **Producer Functionality**: Test message production with various payload sizes
- **Consumer Group Operations**: Test consumer group rebalancing and offset management
- **Schema Evolution**: Test Protocol Buffers schema compatibility and evolution

### Performance Testing
- **Throughput Testing**: Achieve target throughput of 100K+ messages/second
- **Latency Testing**: Verify sub-10ms end-to-end latency for critical paths
- **Load Testing**: Test under realistic trading workload scenarios
- **Stress Testing**: Test cluster behavior under extreme load conditions

### Reliability Testing
- **Fault Tolerance**: Test broker failure and recovery scenarios
- **Data Durability**: Verify message persistence and replication
- **Consumer Recovery**: Test consumer failure and restart scenarios
- **Network Partition**: Test behavior during network splits

### Integration Testing
- **Service Integration**: Test message flow between all microservices
- **Schema Compatibility**: Test schema evolution across service updates
- **Dead Letter Queue**: Test failed message handling and DLQ processing
- **Monitoring Integration**: Verify all metrics and alerts are working

## Claude Code Instructions

```
When implementing this feature:
1. Use existing infrastructure/kafka/ directory with Docker Compose configuration for 3-broker cluster
2. Initially deploy with local storage under ./data/, prepare migration path to /mnt/ssd4tb when SSD is available
3. Deploy Confluent Schema Registry alongside Kafka for schema management
4. Use existing topic naming convention (market-data.krx.*, orders.*, signals.*, etc.) from setup-topics.sh
5. Apply Linux kernel tuning parameters from kafka-startup-guide.md for optimal performance
6. Create Protocol Buffers schema definitions with Schema Registry integration
7. Implement high-performance producer service with batching, LZ4 compression, and retry logic
8. Create robust consumer service with batch processing, error handling, and proper offset management
9. Set up stream processing topology for real-time data transformation and aggregation
10. Deploy Kafka UI for visual monitoring and management at port 8080
11. Implement backup strategy with daily exports to NAS for critical topics
12. Create troubleshooting scripts for common operational issues
13. Use Prometheus Kafka exporter for metrics collection
14. Test with kafka-producer-perf-test and kafka-consumer-perf-test tools
15. For production, implement SASL/SSL authentication and increase heap to 8GB per broker
```

## Notes

- Existing Kafka infrastructure in place with working Docker Compose configuration
- Dual storage strategy: initial local deployment, production on 4TB NVMe SSD
- Topic naming follows existing convention: market-data.krx.*, orders.*, signals.*
- Confluent Schema Registry provides centralized schema management
- Kafka UI at port 8080 provides visual cluster monitoring
- Replication factor of 2 balances durability with resource usage
- Korean market (KRX) and cryptocurrency market topics included
- Comprehensive Linux kernel tuning parameters documented
- Backup and recovery procedures established for critical topics
- Prometheus integration ready for production monitoring

## Status Updates

- **2025-08-24**: Feature specification created with comprehensive Kafka setup architecture
- **2025-08-28**: Updated spec with existing infrastructure configuration from brainstorming files